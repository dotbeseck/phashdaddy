import itertools
import requests
import datetime
from bs4 import BeautifulSoup
from PIL import Image
import imagehash
import concurrent.futures
import sys
import os
import io
import dnstwist
from tqdm import tqdm

# Commonly Used Words in Phishing sites
word_list = [
    "PUTWORDSHERE",
]
# They just keep iterating with word + number so look at alot
number_list = list(range(1, 12000))
dns_number_list = list(range(1, 1000))
# set domain to target
maindomain = "godaddysites.com"

# list of images in the same directory as this script to use with phash...need more
known_images = [
    "LIST OF KNOWN IMAGES TO PHASH",
]
# hash above images with phash for comparison
known_phashes = [imagehash.average_hash(Image.open(img)) for img in known_images]
# initiate dnstwist generated domains
lookalike_domains = []


# This whole class sole purpose is to not printthe list generated by dnstwist...
class Suppress_domain_list:
    def __enter__(self):
        self.original_stdout = sys.stdout
        sys.stdout = open(os.devnull, "w")

    def __exit__(self, exc_type, exc_val, exc_tb):
        sys.stdout.close()
        sys.stdout = self.original_stdout


# Use dnstwist to get a good list of squatted domains using its fuzzers
with Suppress_domain_list():
    domain_variants = dnstwist.run(
        domain="YOURDOMAIN.com",
        registered=False,
        format="list",
        fuzzers="addition,bitsquatting,dictionary,homoglyph,insertion,repetition,transposition,vowel-swap",
        dictionary="phishWords.dict",
    )
    pass

# ignore these generated subdomains
for domain in domain_variants:
    if domain["domain"].startswith("xn--"):
        continue
    domain, _ = domain["domain"].split(".", 1)
    if domain.startswith("ch"):
        lookalike_domains.append(domain)

# initialize a set of domains, used to make sure we only get unique domains
matched_domains = set()
file = f"gdphishing_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"


# The working bit
def check_url(url):
    try:
        # get domain for the set above
        domain = url.split("/")[2]
        with requests.get(url) as response:
            # only care if domain is live
            if response.status_code == 200:
                # These variables get all the data for the images from the gd sites
                soup = BeautifulSoup(response.content, "html.parser")
                images = soup.find_all("meta", property="og:image")
                # iterate all found images
                for image in images:
                    img_url = image.get("content")
                    img_response = requests.get(img_url)
                    img = Image.open(io.BytesIO(img_response.content))
                    # compare the hashes
                    img_phash = imagehash.average_hash(img)
                    for known_phash in known_phashes:
                        # Seems to be the sweetspot so we dont trigger on other companies...
                        if img_phash - known_phash < 9:
                            if domain not in matched_domains:
                                matched_domains.add(domain)
                                with open(file, "a") as f:
                                    f.write(url + "\n")
                            return
    # continue if we hit some exception
    except Exception as e:
        pass
        # print(f"Error on {url}: {e}") ...for checking errors


urls1 = [
    f"http://{subdomain}{number}.{maindomain}"
    for subdomain in lookalike_domains
    for number in dns_number_list
]
urls2 = [
    f"http://{word}{number}.{maindomain}"
    for word in word_list
    for number in number_list
]
# concat the lists together (I will never forget how)
urls = urls1 + urls2
# do some threading to make it go quicker and a progress bar...mainly for troubleshooting.
with concurrent.futures.ThreadPoolExecutor(max_workers=30) as executor:
    list(tqdm(executor.map(check_url, urls), total=len(urls)))
